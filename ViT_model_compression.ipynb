{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1uI49D1AVxl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "import torch.quantization\n",
        "from torchvision.models import vision_transformer as ViT\n",
        "\n",
        "\n",
        "class TeacherViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherViT, self).__init__()\n",
        "        self.vit = models.vit_b_16(weights=ViT.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "        self.vit.heads = nn.Linear(self.vit.hidden_dim, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "class StudentViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentViT, self).__init__()\n",
        "        self.vit = models.vit_b_16()\n",
        "        self.vit.heads = nn.Linear(self.vit.hidden_dim, 10)\n",
        "        # decrease num of transformer blocks\n",
        "        self.vit.encoder.layers = self.vit.encoder.layers[:6]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, temperature=2.0):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.kl_div = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        kd_loss = self.kl_div(\n",
        "            torch.log_softmax(student_logits / self.temperature, dim=1),\n",
        "            torch.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        ) * (self.temperature ** 2)\n",
        "        ce_loss = self.ce_loss(student_logits, labels)\n",
        "        return self.alpha * kd_loss + (1 - self.alpha) * ce_loss\n",
        "\n",
        "def train(teacher_model, student_model, train_loader, optimizer, distillation_loss, device):\n",
        "    teacher_model.eval()\n",
        "    student_model.train()\n",
        "    print(\"Length: \", len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            teacher_output = teacher_model(data)\n",
        "        student_output = student_model(data)\n",
        "        loss = distillation_loss(student_output, teacher_output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "          print(f\"Batch {batch_idx+1}, Loss: {loss.item()}\")\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "    return 100. * correct / total\n",
        "\n",
        "def prune_model(model, amount=0.3):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "    return model\n",
        "\n",
        "def qat_model(model):\n",
        "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "    # Only prepare the linear modules for quantization\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.quantization.prepare_qat(module, inplace=True)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M70A_h9CJ0Iv",
        "outputId": "ede399cb-3b09-41b1-ea69-451a0c2f8c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Data loaded successfully!\n",
            "Models initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_subset = data_utils.Subset(trainset, range(8000))\n",
        "test_subset = data_utils.Subset(trainset, range(2000))\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
        "print(\"Data loaded successfully!\")\n",
        "\n",
        "teacher_model = TeacherViT().to(device)\n",
        "student_model = StudentViT().to(device)\n",
        "print(\"Models initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcEDWHQFAW6I",
        "outputId": "de3cc617-f36d-4680-a2e9-6a5c8e1b3af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length:  125\n",
            "Batch 1, Loss: 1.3523688316345215\n",
            "Batch 11, Loss: 1.4267396926879883\n",
            "Batch 21, Loss: 1.241178274154663\n",
            "Batch 31, Loss: 1.2082445621490479\n",
            "Batch 41, Loss: 1.1923753023147583\n",
            "Batch 51, Loss: 1.1969530582427979\n",
            "Batch 61, Loss: 1.1723254919052124\n",
            "Batch 71, Loss: 1.2015101909637451\n",
            "Batch 81, Loss: 1.1453453302383423\n",
            "Batch 91, Loss: 1.1224390268325806\n",
            "Batch 101, Loss: 1.1280912160873413\n",
            "Batch 111, Loss: 1.1324542760849\n",
            "Batch 121, Loss: 1.1429380178451538\n",
            "Epoch 1, Accuracy: 19.15%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1268446445465088\n",
            "Batch 11, Loss: 1.162122130393982\n",
            "Batch 21, Loss: 1.1351104974746704\n",
            "Batch 31, Loss: 1.1078426837921143\n",
            "Batch 41, Loss: 1.1341239213943481\n",
            "Batch 51, Loss: 1.1478160619735718\n",
            "Batch 61, Loss: 1.128321647644043\n",
            "Batch 71, Loss: 1.1012498140335083\n",
            "Batch 81, Loss: 1.1305257081985474\n",
            "Batch 91, Loss: 1.1339735984802246\n",
            "Batch 101, Loss: 1.0814881324768066\n",
            "Batch 111, Loss: 1.2038604021072388\n",
            "Batch 121, Loss: 1.1270062923431396\n",
            "Epoch 2, Accuracy: 23.6%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.139831781387329\n",
            "Batch 11, Loss: 1.1304787397384644\n",
            "Batch 21, Loss: 1.1047754287719727\n",
            "Batch 31, Loss: 1.1091665029525757\n",
            "Batch 41, Loss: 1.110457420349121\n",
            "Batch 51, Loss: 1.1191916465759277\n",
            "Batch 61, Loss: 1.128088116645813\n",
            "Batch 71, Loss: 1.167710781097412\n",
            "Batch 81, Loss: 1.1116340160369873\n",
            "Batch 91, Loss: 1.1197019815444946\n",
            "Batch 101, Loss: 1.118251919746399\n",
            "Batch 111, Loss: 1.1155939102172852\n",
            "Batch 121, Loss: 1.1584941148757935\n",
            "Epoch 3, Accuracy: 17.6%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1227232217788696\n",
            "Batch 11, Loss: 1.082281470298767\n",
            "Batch 21, Loss: 1.1241106986999512\n",
            "Batch 31, Loss: 1.143622875213623\n",
            "Batch 41, Loss: 1.1321656703948975\n",
            "Batch 51, Loss: 1.1036393642425537\n",
            "Batch 61, Loss: 1.1424109935760498\n",
            "Batch 71, Loss: 1.0860459804534912\n",
            "Batch 81, Loss: 1.1251003742218018\n",
            "Batch 91, Loss: 1.1542892456054688\n",
            "Batch 101, Loss: 1.1233707666397095\n",
            "Batch 111, Loss: 1.1115210056304932\n",
            "Batch 121, Loss: 1.1425297260284424\n",
            "Epoch 4, Accuracy: 21.25%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1279889345169067\n",
            "Batch 11, Loss: 1.1385473012924194\n",
            "Batch 21, Loss: 1.1216020584106445\n",
            "Batch 31, Loss: 1.1232365369796753\n",
            "Batch 41, Loss: 1.168958306312561\n",
            "Batch 51, Loss: 1.0884110927581787\n",
            "Batch 61, Loss: 1.185704231262207\n",
            "Batch 71, Loss: 1.1378376483917236\n",
            "Batch 81, Loss: 1.1393178701400757\n",
            "Batch 91, Loss: 1.121653437614441\n",
            "Batch 101, Loss: 1.0935202836990356\n",
            "Batch 111, Loss: 1.0852253437042236\n",
            "Batch 121, Loss: 1.1153857707977295\n",
            "Epoch 5, Accuracy: 24.8%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1358364820480347\n",
            "Batch 11, Loss: 1.04535710811615\n",
            "Batch 21, Loss: 1.1361843347549438\n",
            "Batch 31, Loss: 1.1412265300750732\n",
            "Batch 41, Loss: 1.1543054580688477\n",
            "Batch 51, Loss: 1.1210157871246338\n",
            "Batch 61, Loss: 1.1943182945251465\n",
            "Batch 71, Loss: 1.153039574623108\n",
            "Batch 81, Loss: 1.1525506973266602\n",
            "Batch 91, Loss: 1.1290255784988403\n",
            "Batch 101, Loss: 1.0959903001785278\n",
            "Batch 111, Loss: 1.1095243692398071\n",
            "Batch 121, Loss: 1.0961323976516724\n",
            "Epoch 6, Accuracy: 22.8%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1315314769744873\n",
            "Batch 11, Loss: 1.0856781005859375\n",
            "Batch 21, Loss: 1.1218687295913696\n",
            "Batch 31, Loss: 1.0697999000549316\n",
            "Batch 41, Loss: 1.1100469827651978\n",
            "Batch 51, Loss: 1.128303050994873\n",
            "Batch 61, Loss: 1.087416410446167\n",
            "Batch 71, Loss: 1.0638521909713745\n",
            "Batch 81, Loss: 1.0964622497558594\n",
            "Batch 91, Loss: 1.119051218032837\n",
            "Batch 101, Loss: 1.0827726125717163\n",
            "Batch 111, Loss: 1.1128405332565308\n",
            "Batch 121, Loss: 1.1419895887374878\n",
            "Epoch 7, Accuracy: 24.5%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.092366099357605\n",
            "Batch 11, Loss: 1.1497576236724854\n",
            "Batch 21, Loss: 1.163444995880127\n",
            "Batch 31, Loss: 1.0992140769958496\n",
            "Batch 41, Loss: 1.0761313438415527\n",
            "Batch 51, Loss: 1.1649909019470215\n",
            "Batch 61, Loss: 1.0333261489868164\n",
            "Batch 71, Loss: 1.1244033575057983\n",
            "Batch 81, Loss: 1.1230813264846802\n",
            "Batch 91, Loss: 1.11624276638031\n",
            "Batch 101, Loss: 1.1318445205688477\n",
            "Batch 111, Loss: 1.1027169227600098\n",
            "Batch 121, Loss: 1.0896222591400146\n",
            "Epoch 8, Accuracy: 24.95%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1284841299057007\n",
            "Batch 11, Loss: 1.1460487842559814\n",
            "Batch 21, Loss: 1.1023454666137695\n",
            "Batch 31, Loss: 1.116987943649292\n",
            "Batch 41, Loss: 1.039962649345398\n",
            "Batch 51, Loss: 1.1084603071212769\n",
            "Batch 61, Loss: 1.1155468225479126\n",
            "Batch 71, Loss: 1.1586782932281494\n",
            "Batch 81, Loss: 1.0952038764953613\n",
            "Batch 91, Loss: 1.113470196723938\n",
            "Batch 101, Loss: 1.0983439683914185\n",
            "Batch 111, Loss: 1.1408754587173462\n",
            "Batch 121, Loss: 1.0790656805038452\n",
            "Epoch 9, Accuracy: 25.25%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.130287528038025\n",
            "Batch 11, Loss: 1.1082762479782104\n",
            "Batch 21, Loss: 1.0802569389343262\n",
            "Batch 31, Loss: 1.0789037942886353\n",
            "Batch 41, Loss: 1.0952990055084229\n",
            "Batch 51, Loss: 1.102983832359314\n",
            "Batch 61, Loss: 1.0731385946273804\n",
            "Batch 71, Loss: 1.0596591234207153\n",
            "Batch 81, Loss: 1.101671576499939\n",
            "Batch 91, Loss: 1.0652894973754883\n",
            "Batch 101, Loss: 1.0533325672149658\n",
            "Batch 111, Loss: 1.1198503971099854\n",
            "Batch 121, Loss: 1.0979775190353394\n",
            "Epoch 10, Accuracy: 25.8%\n"
          ]
        }
      ],
      "source": [
        "# Knowledge distillation\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "distillation_loss = DistillationLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(teacher_model, student_model, train_loader, optimizer, distillation_loss, device)\n",
        "    accuracy = evaluate(student_model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EICeeAIFAXDF",
        "outputId": "172c82bf-4dd3-46ca-a352-5bb359f0c7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned model accuracy: 26.3%\n"
          ]
        }
      ],
      "source": [
        "pruned_model = prune_model(student_model)\n",
        "accuracy = evaluate(pruned_model, test_loader, device)\n",
        "print(f\"Pruned model accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbETJwYsAXFa",
        "outputId": "86de3395-3d40-43db-9445-857d755d9791"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize.py:320: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
            "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length:  125\n",
            "Batch 1, Loss: 1.316054344177246\n",
            "Batch 11, Loss: 1.2011091709136963\n",
            "Batch 21, Loss: 1.1418251991271973\n",
            "Batch 31, Loss: 1.1504271030426025\n",
            "Batch 41, Loss: 1.1098215579986572\n",
            "Batch 51, Loss: 1.1115397214889526\n",
            "Batch 61, Loss: 1.1399153470993042\n",
            "Batch 71, Loss: 1.0721808671951294\n",
            "Batch 81, Loss: 1.1016970872879028\n",
            "Batch 91, Loss: 1.1157045364379883\n",
            "Batch 101, Loss: 1.1227526664733887\n",
            "Batch 111, Loss: 1.031515121459961\n",
            "Batch 121, Loss: 1.1034893989562988\n",
            "QAT Epoch 1, Accuracy: 28.15%\n",
            "Length:  125\n",
            "Batch 1, Loss: 1.1279518604278564\n",
            "Batch 11, Loss: 1.1133867502212524\n",
            "Batch 21, Loss: 1.0771567821502686\n",
            "Batch 31, Loss: 1.024730920791626\n",
            "Batch 41, Loss: 1.0906916856765747\n",
            "Batch 51, Loss: 1.0369982719421387\n",
            "Batch 61, Loss: 1.0119776725769043\n",
            "Batch 71, Loss: 1.0649549961090088\n",
            "Batch 81, Loss: 1.0364967584609985\n",
            "Batch 91, Loss: 1.0247160196304321\n",
            "Batch 101, Loss: 1.014143466949463\n",
            "Batch 111, Loss: 1.048240065574646\n",
            "Batch 121, Loss: 1.0948542356491089\n",
            "QAT Epoch 2, Accuracy: 38.6%\n",
            "Length:  125\n",
            "Batch 1, Loss: 0.9619466066360474\n",
            "Batch 11, Loss: 1.0863105058670044\n",
            "Batch 21, Loss: 1.0328654050827026\n",
            "Batch 31, Loss: 1.0410913228988647\n",
            "Batch 41, Loss: 1.0508359670639038\n",
            "Batch 51, Loss: 1.0947250127792358\n",
            "Batch 61, Loss: 1.0193239450454712\n",
            "Batch 71, Loss: 0.9752020239830017\n",
            "Batch 81, Loss: 0.9915907382965088\n",
            "Batch 91, Loss: 0.973641037940979\n",
            "Batch 101, Loss: 1.057062029838562\n",
            "Batch 111, Loss: 0.977026104927063\n",
            "Batch 121, Loss: 0.9827369451522827\n",
            "QAT Epoch 3, Accuracy: 42.35%\n",
            "Length:  125\n",
            "Batch 1, Loss: 0.9702746868133545\n",
            "Batch 11, Loss: 0.9729017019271851\n",
            "Batch 21, Loss: 0.9715509414672852\n",
            "Batch 31, Loss: 1.0043524503707886\n",
            "Batch 41, Loss: 0.9587534666061401\n",
            "Batch 51, Loss: 1.0025155544281006\n",
            "Batch 61, Loss: 0.9585995674133301\n",
            "Batch 71, Loss: 0.9903789758682251\n",
            "Batch 81, Loss: 1.0105509757995605\n",
            "Batch 91, Loss: 0.9372225999832153\n",
            "Batch 101, Loss: 0.954582691192627\n",
            "Batch 111, Loss: 0.9964471459388733\n",
            "Batch 121, Loss: 0.9599286317825317\n",
            "QAT Epoch 4, Accuracy: 50.15%\n",
            "Length:  125\n",
            "Batch 1, Loss: 0.8843587636947632\n",
            "Batch 11, Loss: 0.9289417266845703\n",
            "Batch 21, Loss: 0.978430986404419\n",
            "Batch 31, Loss: 0.9431454539299011\n",
            "Batch 41, Loss: 1.029323935508728\n",
            "Batch 51, Loss: 0.9005064368247986\n",
            "Batch 61, Loss: 0.9755970239639282\n",
            "Batch 71, Loss: 0.9788020849227905\n",
            "Batch 81, Loss: 0.925447404384613\n",
            "Batch 91, Loss: 0.9782587289810181\n",
            "Batch 101, Loss: 0.9021844863891602\n",
            "Batch 111, Loss: 0.9223313331604004\n",
            "Batch 121, Loss: 0.9635406732559204\n",
            "QAT Epoch 5, Accuracy: 51.55%\n"
          ]
        }
      ],
      "source": [
        "# QAT\n",
        "pruned_model = StudentViT().to(device)\n",
        "qat_model = qat_model(pruned_model.train())\n",
        "optimizer = optim.Adam(qat_model.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(teacher_model, qat_model, train_loader, optimizer, distillation_loss, device)\n",
        "    accuracy = evaluate(qat_model, test_loader, device)\n",
        "    print(f\"QAT Epoch {epoch+1}, Accuracy: {accuracy}%\")\n",
        "\n",
        "quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
        "\n",
        "# Save model\n",
        "torch.jit.save(torch.jit.script(quantized_model), \"quantized_vit.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RbyUUtBKa3v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
